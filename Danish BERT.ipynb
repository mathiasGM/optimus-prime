{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing with transformer (Danish BERT) \n",
    "This is an implementation of a pre-trained transformer model (BERT).\n",
    "\n",
    "The weights were provided by Mollerhoj from https://github.com/botxo/nordic_bert\n",
    "\n",
    "The conversion of the weights were done with help from Daniel Varab https://github.com/danielvarab/convert_da_bert \n",
    "\n",
    "This notebook shows how transformers can be used for tokenization, maskedLM and writing sentences.\n",
    "\n",
    "To run this notebook, first download the 'bert-base-danish-uncased-v2' folder, which I have made available here: https://drive.google.com/drive/folders/1TsttvxSAjwJHKu5Onhq7KSEADU5yS7T1?usp=sharing\n", 
    "\n",
    "//By MGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained transformer (BERT) model and tokenizer \n",
    "model = transformers.BertForMaskedLM.from_pretrained('bert-base-danish-uncased-v2')\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-danish-uncased-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of tokienization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['dansk', 'er', 'et', 'skønt', 'sprog']\n",
      "\n",
      "Token indices:\n",
      "[674, 33, 100, 5192, 1968]\n"
     ]
    }
   ],
   "source": [
    "txt = \"Dansk er et skønt sprog\"\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "print(\"\")\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token indices:\")\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict word(s) using MaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I dag er en [MASK] dag . [SEP]\n",
      "Predicted token: stor\n"
     ]
    }
   ],
   "source": [
    "# Inspired by Daniel Varab\n",
    "\n",
    "txt_input = \"I dag er en [MASK] dag .\"\n",
    "\n",
    "# Add masking\n",
    "sentence_input = \"[CLS] \" + txt_input + \" [SEP]\"\n",
    "print(sentence_input)\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence_input)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# Get index of masking\n",
    "mask_index = tokens.index(\"[MASK]\")\n",
    "\n",
    "# Convert tokens to a tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_ids = [0] * len(indexed_tokens)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# We don't want PyTorch to calculate the gradients\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, mask_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "\n",
    "print(\"Predicted token: \" + predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: stor\n",
      "2: fantastisk\n",
      "3: dejlig\n",
      "4: god\n",
      "5: fars\n",
      "6: perfekt\n",
      "7: anden\n",
      "8: mors\n",
      "9: lang\n",
      "10: ny\n"
     ]
    }
   ],
   "source": [
    "# Print the top k=10 predicted words\n",
    "for i, idx in enumerate(torch.topk(predictions[0, mask_index], k=10)[1]):\n",
    "    print(\"%d:\" % (i+1), tokenizer.convert_ids_to_tokens(idx.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write with danish BERT transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........\n",
      "Input: Jeg ønsker at \n",
      "...........\n",
      "Output: Jeg ønsker at leve livet, som det er, og leve det med mening og indhold i \n"
     ]
    }
   ],
   "source": [
    "sentence = \"Jeg ønsker at \"\n",
    "\n",
    "print(\"...........\")\n",
    "print(\"Input: \" + sentence)\n",
    "print(\"...........\")\n",
    "\n",
    "# Number of words that should be written\n",
    "n = 15\n",
    "\n",
    "\n",
    "sentence_input = \"[CLS] \" + sentence + \" [MASK]\"\n",
    "\n",
    "for i in range(n):\n",
    "    tokens = tokenizer.tokenize(sentence_input)\n",
    "    mask_index = tokens.index(\"[MASK]\")\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_ids = [0] * len(indexed_tokens)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    # Top predicted token\n",
    "    predicted_index = torch.argmax(predictions[0, mask_index]).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(predicted_index) \n",
    "    \n",
    "    # Set punctuating right\n",
    "    if predicted_token in \".,\":\n",
    "        \n",
    "        \n",
    "        # If the model repeats itself with ',.' We can choose the second predicted word to avoid this.\n",
    "        \"\"\"\n",
    "        if predicted_token in sentence[-2:]:\n",
    "            predicted_index = torch.topk(predictions[0, mask_index], k=10)[1][1].item()\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "            #sentence = sentence + predicted_token + \" \"\n",
    "            #sentence_input = \"[CLS] \" + sentence + \"[MASK]\"\n",
    "            continue\n",
    "        \"\"\"\n",
    "        sentence = sentence.rstrip() + predicted_token + \" \"\n",
    "        sentence_input = \"[CLS] \" + sentence + \"[MASK]\"\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    sentence = sentence + predicted_token + \" \"\n",
    "    sentence_input = \"[CLS] \" + sentence + \"[MASK]\"\n",
    "\n",
    "print(\"Output: \" + sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
